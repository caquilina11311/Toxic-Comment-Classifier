{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "66e10433-1d96-44a5-9ff0-1e43dc02695f",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'numpy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Sequential\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlayers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Embedding, Conv1D, GlobalMaxPooling1D, Dense, MaxPooling1D, Dropout\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'numpy'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, Conv1D, GlobalMaxPooling1D, Dense, MaxPooling1D, Dropout\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.datasets import imdb\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('stopwords')\n",
    "import re\n",
    "seed = 1234\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4206a66c-78a2-4cb6-9580-d76c4693f7ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "159571\n",
      "119678\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('train.csv')\n",
    "is_toxic = (df[['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']] == 0).all(axis=1)\n",
    "not_toxic = [d for d in range(0, len(df)) if is_toxic[d]]\n",
    "toxic = [d for d in range(0, len(df)) if not is_toxic[d]]\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(df[\"comment_text\"],is_toxic)\n",
    "print(len(df))\n",
    "print(len(X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2bc47137-03bc-4426-a5ff-345b2a78d798",
   "metadata": {},
   "outputs": [],
   "source": [
    "swords = stopwords.words(\"english\")\n",
    "swords.append(\"im\")\n",
    "\n",
    "\n",
    "def modify_input(tokens):\n",
    "    new_list = []\n",
    "    for token in tokens:\n",
    "        if token not in swords:\n",
    "            word = re.sub(r'[^A-Za-z0-9\\s]', '', token)\n",
    "            if(len(word) > 0):\n",
    "                new_list.append(word)\n",
    "    return new_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "633ee662-b6e1-4b03-a834-432545a4de33",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = [word_tokenize(sen.lower().strip()) for sen in X_train]\n",
    "clean_tokens = []\n",
    "for token_list in tokens:\n",
    "    clean_tokens.append(modify_input(token_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f2ef88fe-59a8-4526-98ff-b02018de4d0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(119678, 100)\n"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "from keras.models import Sequential\n",
    "df = pd.read_csv('train.csv')\n",
    "is_toxic = (df[['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']] == 0).all(axis=1)\n",
    "not_toxic = [d for d in range(0, len(df)) if is_toxic[d]]\n",
    "toxic = [d for d in range(0, len(df)) if not is_toxic[d]]\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(df[\"comment_text\"],is_toxic)\n",
    "# Tokenization using Keras Tokenizer\n",
    "tokenizer = Tokenizer(num_words=20000)  # Limit vocabulary size\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "tokenizer.fit_on_texts(X_test)\n",
    "train_sequences = tokenizer.texts_to_sequences(X_train)\n",
    "test_sequences = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "# Padding sequences to ensure same length\n",
    "max_seq_len = 100\n",
    "train_padded = pad_sequences(train_sequences, maxlen=max_seq_len)\n",
    "test_padded = pad_sequences(test_sequences, maxlen=max_seq_len)\n",
    "print(train_padded.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "371d8f32-660a-44ec-bdb2-1cbbe623d2f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "FBetaScore expects 2D inputs with shape (batch_size, output_dim). Received input shapes: y_pred.shape=(None, 1) and y_true.shape=(None,).",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 20\u001b[39m\n\u001b[32m     15\u001b[39m model.add(Dense(\u001b[32m1\u001b[39m, activation=\u001b[33m'\u001b[39m\u001b[33msigmoid\u001b[39m\u001b[33m'\u001b[39m))  \u001b[38;5;66;03m# For binary classification\u001b[39;00m\n\u001b[32m     16\u001b[39m model.compile(optimizer=\u001b[33m'\u001b[39m\u001b[33madam\u001b[39m\u001b[33m'\u001b[39m, loss=\u001b[33m'\u001b[39m\u001b[33mbinary_crossentropy\u001b[39m\u001b[33m'\u001b[39m, metrics=[\u001b[33m'\u001b[39m\u001b[33maccuracy\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m     17\u001b[39m                               tf.keras.metrics.Precision(),\n\u001b[32m     18\u001b[39m                               tf.keras.metrics.Recall(),\n\u001b[32m     19\u001b[39m                               tf.metrics.F1Score()])\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_padded\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_padded\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY_test\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m32\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py:122\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    119\u001b[39m     filtered_tb = _process_traceback_frames(e.__traceback__)\n\u001b[32m    120\u001b[39m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[32m    121\u001b[39m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m122\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e.with_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    123\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    124\u001b[39m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/keras/src/metrics/f_score_metrics.py:124\u001b[39m, in \u001b[36mFBetaScore._build\u001b[39m\u001b[34m(self, y_true_shape, y_pred_shape)\u001b[39m\n\u001b[32m    122\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[34m_build\u001b[39m(\u001b[38;5;28mself\u001b[39m, y_true_shape, y_pred_shape):\n\u001b[32m    123\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(y_pred_shape) != \u001b[32m2\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(y_true_shape) != \u001b[32m2\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m124\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    125\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mFBetaScore expects 2D inputs with shape \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    126\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33m(batch_size, output_dim). Received input \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    127\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mshapes: y_pred.shape=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00my_pred_shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m and \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    128\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33my_true.shape=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00my_true_shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    129\u001b[39m         )\n\u001b[32m    130\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m y_pred_shape[-\u001b[32m1\u001b[39m] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m y_true_shape[-\u001b[32m1\u001b[39m] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    131\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    132\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mFBetaScore expects 2D inputs with shape \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    133\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33m(batch_size, output_dim), with output_dim fully \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m    136\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33my_true.shape=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00my_true_shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    137\u001b[39m         )\n",
      "\u001b[31mValueError\u001b[39m: FBetaScore expects 2D inputs with shape (batch_size, output_dim). Received input shapes: y_pred.shape=(None, 1) and y_true.shape=(None,)."
     ]
    }
   ],
   "source": [
    "# model = Sequential([\n",
    "#   Embedding(39450, 16),\n",
    "#   Dropout(0.2),\n",
    "#   GlobalAveragePooling1D(),\n",
    "#   Dropout(0.2),\n",
    "#   Dense(1, activation='sigmoid')])\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(20000, 100))\n",
    "#model.add(Dense(128, activation = 'relu'))\n",
    "model.add(Conv1D(filters = 256, kernel_size = 3, activation = 'relu'))\n",
    "model.add(GlobalMaxPooling1D())\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))  # For binary classification\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy',\n",
    "                              tf.keras.metrics.Precision(),\n",
    "                              tf.keras.metrics.Recall(),\n",
    "                              tf.metrics.F1Score()])\n",
    "model.fit(train_padded, Y_train, validation_data = (test_padded, Y_test), batch_size = 32, epochs = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "fd651751-342f-4b71-8000-a51789eee6c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: [[0.11291726678609848, 0.9607951045036316, 0.9673316478729248, 0.9897517561912537, 0.946079432964325]]\n"
     ]
    }
   ],
   "source": [
    "accuracy = [model.evaluate(test_padded, Y_test, verbose=0)]\n",
    "print(f\"Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ccaca1d0-6111-4680-aee8-2b605584660d",
   "metadata": {},
   "outputs": [],
   "source": [
    "testl = pd.read_csv('test.csv')\n",
    "testv = pd.read_csv('test_labels.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "8d03e7ba-f3f9-4201-8d45-a301fb82b8d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0         False\n",
      "1         False\n",
      "2         False\n",
      "3         False\n",
      "4         False\n",
      "          ...  \n",
      "153159    False\n",
      "153160    False\n",
      "153161    False\n",
      "153162    False\n",
      "153163    False\n",
      "Length: 153164, dtype: bool\n"
     ]
    }
   ],
   "source": [
    "tokenizer.fit_on_texts(testl[\"comment_text\"])\n",
    "tl = tokenizer.texts_to_sequences(testl[\"comment_text\"])\n",
    "max_seq_len = 100\n",
    "tlp = pad_sequences(tl, maxlen=max_seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "5655394e-72f4-4f52-9930-bca395069e58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0         False\n",
      "1         False\n",
      "2         False\n",
      "3         False\n",
      "4         False\n",
      "          ...  \n",
      "153159    False\n",
      "153160    False\n",
      "153161    False\n",
      "153162    False\n",
      "153163    False\n",
      "Length: 153164, dtype: bool\n",
      ":If you have a look back at the source, the information I updated was the correct form. I can only guess the source hadn't updated. I shall update the information once again but thank you for your message.\n"
     ]
    }
   ],
   "source": [
    "is_toxic = (testv[['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']] == 0).all(axis=1)\n",
    "print(is_toxic)\n",
    "toxic = [d for d in range(0, len(df)) if not is_toxic[d]]\n",
    "print(testl[\"comment_text\"][3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "2c3ebc4f-a157-4766-9158-f23aabb1c58f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.4027186632156372\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(tlp,is_toxic, verbose=0)\n",
    "print(f\"Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94e08699-9a74-46c8-9030-6af8ee05d43a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "116",
   "language": "python",
   "name": "116"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
