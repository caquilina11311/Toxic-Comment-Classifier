# Toxic-Comment-Classifier

Dataset:
The dataset that was chosen was the Toxic Comment dataset. This dataset contains 159571 samples of small comments with each comment having eight columns. The first two columns for each sample contain information about the sample, with the first column containing the unique ID of the comment and the second column containing the text of the comment. The next six columns are labels corresponding to the level of toxicity of the comment. A comment can be classified as toxic, severely toxic, obscene, threatening, insulting, or containing speech pertaining to identity hate. The number one is used to denote whether a comment is classified as one of these labels, with the number zero indicating that the label does not classify the comment. With this in mind, the dataset was then divided into two segments, one with comments that were not classified with any of the labels and one where the comment was classified with at least one of the labels. The segment with all of the columns having the value of zero were labeled as not harmful, whereas the segment with at least one column having a value of one was given the label of harmful
	As seen above, of the 159571 comments present in the dataset, only around ten percent or 16000 of the comments were considered to be harmful, with the remaining ninety percent being not harmful. Compared to the size of the dataset, this is a relatively low number of comments that are considered to be harmful. Observing the data of the harmful comments further, it was decided to see what label occurs the most in the 16000 comment sample. 
The above graph displays how common each of the six types is among the 16000 sized sample of harmful comments. As a comment can be classified as more than one of the labels, some of the comments in the graph above may be in more than one category. However, this graph reveals that of the six different labels, the sample contains the most comments that are considered toxic, with about 43% of the 16000 comments being classified as such. Following this label is the Obscene label with about 24% of the comment sample being classified as it. The least common label present in the harmful comments is the threat label, with only 1.4% of the 16000 comments having this classification or only 224 comments in total. As discussed below, this data was then used to manipulate the inputs to the models.

Input Manipulation:
Before manipulation could be done on the input, all of the 159571 comments were divided into two datasets, one to be used on training the models and one to be used to validate them. The X values for both the validation and testing datasets were taken from the “comment_text” column in the original dataset. For the labels for both datasets, each comment was given the classification of not harmful if it was not classified as any of the six types of harmful comments described above or harmful if it was classified as at least one. This was done because certain types of harmful comments such as Threats appear much less often than the other types. This would make it harder to classify correctly whether or not a comment is of that type because the model will be working off of much less samples. Since the model being trained is one that should be able to classify whether a comment is harmful or not, these six features are redundant as they are all just different types of harmful comments. In addition, by combining all of the harmful types of comments into one feature, this helps decrease the complexity of the model, which can help with controlling overfitting. After combining all of the harmful comment labels into one label, the overall dataset was then split into its corresponding test and validation splits. 75% of the data was put into the training dataset, with the remaining 25% being used to validate the models later. 
Next, before any model could be trained on the data, it is necessary to manipulate the inputs further. There are a couple of techniques that should be applied to the input before training it on the model. First, each comment in both datasets was broken down into a list of individual words. Then, special characters such as exclamation marks and periods were removed and each character in the word was made lowercase. This is because later, when the words are vectorized or converted to numbers, words such as The, the, and the! are not seen as different words when the vectorizer counts the occurrence of each word in the comment. In addition, special characters and whether or not a word has capital letters will not help a model classify a comment more accurately, so this will help reduce the amount of noise in the model by removing them. If, by removing one of the special characters, the resulting word is of length zero, then this empty string will not be considered as a word and will be removed. It is also necessary to remove stop words or words like “a” or “the” from the inputs because they will also not have much of an impact on classifying whether a comment is harmful or not. This will help reduce the amount of noise in the overall model because redundant and useless text such as these words will be removed. This will also reduce the overall time it takes to train the model because the model will not be looking at these words.
Now after filtering out and manipulating the data, a vectorizer can be applied to the words in order to represent them as numbers. This is done because the model can only understand input that is in a numerical form and because the words are strings, this step is critical in order to ensure that the model works correctly. Before applying the vectorizer though, the tokens must be recombined back into the sentences that they were originally in. This is because when the vectorizer is applied to each comment, it will count how many times a word has occurred in the text. This will be helpful with classifying comments as harmful because if swear words appear more in harmful comments, then the model can look for these words in particular to classify whether a comment is harmful or not. A vectorizer will also help identify rare words or words that only appear once or twice throughout the entire dataset. Because of how infrequent these words are in the dataset, they will not really help with classification and can be avoided, which will decrease the time it takes to train the model and it will help reduce the noise of it. With this in mind, the vectorizer will be applied to the tokenized train and validation data so that it can be ready to be used on the model. 

Models:

For this dataset, a Support Vector Machine, Logistic Regression, and a neural network were chosen as possible models. 

Linear Models:
	It was decided that a Soft-Margin Support Vector Machine (SVM) would be chosen over a Hard-Margin SVM for a number of reasons. As some of the words could appear in both harmful and non-harmful comments, this would make it harder to find a definitive separator between harmful and non-harmful comments. In addition, possible noise such as words being spelled incorrectly will not be handled as well in a Hard-Margin SVM due to its tendency to find a definitive separator between the two types of comments. As a result, this will cause the Hard-Margin SVM to be more likely to overfit the data, making it a less effective option than a Soft-Margin SVM.  Next, the parameters for this Soft-Margin SVM will be looked at. Since this model has two classes, is high dimensional and there is not too much noise in the dataset, then the hinge loss function will be used. Because there is a lot of data and not much of the data is mislabeled, then the “optimal” learning rate will be chosen for this model. For this model, a large value for the number of iterations such as 10,000 or more was chosen because of how large the training dataset is. However, with such a large number of iterations, this could cause potential overfitting problems. Therefore, a parameter was set to stop training the model if there was no significant difference in the validation accuracy for the model. The value chosen for this parameter was a low number such as five because it was observed that accuracy starts to slowly decrease as this parameter’s value increases, meaning that the model is more likely to overfit the training data. As a result of these considerations, a large number of iterations, 10,000 was chosen with an early stopping of five if the model’s validation accuracy does not change significantly. With these parameters, this SVM was trained on the training dataset, getting a validation accuracy of around 95% (see table below). 

	In addition to just an SVM, it was decided to also perform a logistic regression classifier on the data as well. This type of learner was chosen because its cost for training is small, meaning that it does not take long to train, the data only has two labels or is binary, the data is represented as high dimensional sparse vectors due to the vectorizer that was applied to the comments earlier, and the model is relatively simple to understand due to the fact that it is not very complex. As a result, the following parameters were used for the model to get a validation accuracy of around 95% (see table below). First, it was decided to apply a regularization penalty on the learner in order to reduce the complexity of the model, which will decrease the model’s likelihood to overfit on the training data. The penalty used was the L2 regularization penalty because since the data is represented as high dimensional vectors as a result of the vectorizer, the L2 regularization penalty will do a better job of decreasing the likelihood of overfitting and generalizing the data, which will increase the accuracy of the model. In addition to just L2 regularization, a value for C, which will control the strength of the regularization, was chosen. Since it is better to have more regularization for this model than less, a low value of C was chosen. It was found after testing multiple values of C that the value of 0.45 gave the highest validation accuracy, so it was chosen for the model. A tolerance of 1e-3 was chosen for this model to control overfitting as the model will stop adjusting its parameters if there is a negligible change in the loss function. This model was also set to run 500 times as there are over 100,000 comments in the training data so this is necessary to allow the model to be able to evaluate the training data effectively. 
Neural Network:
	It was also decided to train the data on a Neural Network model on the data to see how it would perform on a non-linear model. This was done using the TensorFlow library.  However, before the data could be put into a Neural Network, a few more modifications to the input must be done. First, the vocabulary size of the words was limited to 20,000. This was done both to reduce the amount of memory being used by the model and because these 20,000 vocabulary words are the most frequent words in the training data, so by eliminating the other vocabulary words, this is only getting rid of words that appear rarely in the dataset. This means that it will not have a huge impact on the model’s accuracy on the validation data. Since the Neural Network will only really work on inputs of the same length and because the current vectorized word list contains comments of different lengths, it is necessary to apply padding to the vectorized word sequences so that all of the comments are of the same length. A maximum length of 100 was chosen for the length of each vectorized comment in order to make sure that all of the comments will have the same size. 
With these changes done to the input, a Neural Network model can now be constructed. The first layer of the model is an embedded layer in order to reduce dimensionality and to turn words into dense vectors, which will improve the accuracy of the model and the efficiency in which it is trained. The input dimension of the layer would be the length of the vocabulary, which is the 20,000 words described above. An output dimension of 100 was chosen because this data is large. The next layer in the model is a 1D Convolutional layer so that the model can capture local word patterns in the data so that it can better classify comments as being harmful or not. In addition, this will also decrease the time it takes to train the model. The next layer in the model is a 1D Global Max Pooling layer so that only the important features from the previous layer are kept. This helps reduce the complexity of the model, which helps reduce overfitting. A dropout layer with a dropout of twenty percent was also added after this layer in order to reduce overfitting. The next layer, which is the final layer, would be the output layer. This is a Dense layer with one filter and a sigmoid activation function because there are only two labels in the data. When compiling this model, the ‘adam’ optimizer was used with a binary cross entropy loss function because there are only two classes in the data. When fitting the model with the training data, a batch size of 32 was chosen and this model was run for two epochs because the model’s validation accuracy tends to decrease after more than two epochs, which is a sign of overfitting. This resulting model gets a validation accuracy of around 96% (see table below)
Conclusion:
| Model               | Accuracy | Precision | Recall | F1-score | AUC  |
|----------------------|----------|-----------|--------|----------|------|
| CNN (3-layer)        | 96.23%      | 95.38%       | 97.94%    | 97.94%      | 0.95 |
| SVM (RBF kernel)     | 96.11%      | 95.44%       | 97.87%    | 97.51%      | 0.94 |
| SGD (Elastic Net)    | 96.74%      | 96.08%       | 94.60%    | 94.61%      | 0.92 |
| **Ensemble (final)** | **95%**  | **95%**   | **97%**| **97%**  | **0.96** |


As seen in the table above, the Neural network has the largest validation and training accuracies compared to the rest of the models. The Logistic regression model has the worst training accuracy, with the SVM model having the worst validation accuracy. When looking at the F1 scores, all of the models have training and validation scores which are almost identical to each other, signifying that models are both accurately classifying and not overfitting. In terms of the difference between the F1 scores of the training and validation data, the SVM has the largest gap and the Neural Network has the lowest gap. Because the Neural Network model has both the highest validation accuracy and the lowest difference in the training and validation scores, the neural network would be the best model to choose for this classification problem. However, even then, this model has some limitations. For example, if a comment contains certain profanity that appears rarely in the dataset, the model will not accurately classify the comment as harmful. This highlights the apparent bias in the original dataset because certain harmful words are overrepresented over others. In addition, this is worsened when regularization is applied to the models such as with the logistic regression or when only the first 20,000 vocabulary words are used in the training dataset for the Neural Network. But on the contrary, overall variance in the model decreases as a result due to the inverse relationship between bias and variance. 


